{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67116861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import cloudpickle\n",
    "import json\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "COLORMAP = [\"#DFD27F\", \"#C65BAA\", \"#27474F\"]\n",
    "\n",
    "def parityplot(filename, train, test, title=None, \n",
    "               min_max=(-0.3, 0.3), ylabel=\"predicted\", xlabel=\"true\"):\n",
    "    # Start with a square Figure.\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    # Add a gridspec with two rows and two columns and a ratio of 1 to 4 between\n",
    "    # the size of the marginal axes and the main axes in both directions.\n",
    "    # Also adjust the subplot parameters for a square plot.\n",
    "    gs = fig.add_gridspec(2, 2,  width_ratios=(4, 1), height_ratios=(1, 4),\n",
    "                          wspace=0.0, hspace=0.0)\n",
    "    # Create the Axes.\n",
    "    pp = fig.add_subplot(gs[1, 0])\n",
    "    pp_histx = fig.add_subplot(gs[0, 0], sharex=pp)\n",
    "    pp_histy = fig.add_subplot(gs[1, 1], sharey=pp)\n",
    "    pp.plot(min_max, min_max)\n",
    "    pp.scatter([-10e5],[-10e5], marker=\"+\", color=\"black\", alpha=0.6, label=\"training\")\n",
    "    pp.scatter([-10e5],[-10e5], marker=\"+\", color=\"black\", alpha=1, label=\"testing\")\n",
    "    \n",
    "    pp_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    pp_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "    \n",
    "    binwidth = 0.025\n",
    "    bins = np.arange(min_max[0], min_max[1] + binwidth, binwidth)\n",
    "\n",
    "    if len(train) > 2:\n",
    "        true_multi = []\n",
    "        pred_multi = []\n",
    "        plot_colors = []\n",
    "        for idx, label in enumerate(set(train[2]).union(set(test[2]))):\n",
    "            pp.scatter(train[0][train[2] == label],train[1][train[2] == label], \n",
    "                       marker=\"+\", color=COLORMAP[idx],\n",
    "                       alpha=0.6)\n",
    "            pp.scatter(test[0][test[2] == label],test[1][test[2] == label],\n",
    "                       marker=\"+\", color=COLORMAP[idx],\n",
    "                       label=f\"{label}\")\n",
    "            true_multi.append(train[0][train[2] == label])\n",
    "            pred_multi.append(train[1][train[2] == label])\n",
    "            plot_colors.append(COLORMAP[idx]+\"99\")\n",
    "            true_multi.append(test[0][test[2] == label])\n",
    "            pred_multi.append(test[1][test[2] == label])\n",
    "            plot_colors.append(COLORMAP[idx])            \n",
    "            \n",
    "        pp_histx.hist(true_multi, bins, color=plot_colors, stacked=True)\n",
    "        pp_histx.set_ylim((0.2, 25))\n",
    "        pp_histy.hist(pred_multi, bins, color=plot_colors, stacked=True, orientation='horizontal')\n",
    "        pp_histy.set_xlim((0.2, 25))\n",
    "\n",
    "    else:\n",
    "        pp.scatter(train[0],train[1], marker=\"p\", label=\"training\")\n",
    "        pp.scatter(test[0],test[1], marker=\"P\", label=\"testing\")\n",
    "    \n",
    "    pp.legend()\n",
    "    pp.set_ylabel(ylabel)\n",
    "    pp.set_ylim(min_max)\n",
    "    pp.set_xlabel(xlabel)\n",
    "    pp.set_xlim(min_max)\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    fig.clear()\n",
    "\n",
    "def run_regressor_nested_cv(feats, target, model, params,\n",
    "                            sample_class= None, view_class=None,\n",
    "                            test_split=0.2, unit=\"eV\", xlabel=\"$\\mathrm{E}_{\\mathrm{ads}}$\",\n",
    "                            name=\"test\", scaler=None, pp_kws=None):\n",
    "    pp_kws = {} if pp_kws is None else pp_kws\n",
    "    folder = f\"outputs/{name}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "  \n",
    "    if scaler:\n",
    "        scaler = scaler()\n",
    "        scaler.fit(feats)\n",
    "        feats = scaler.transform(feats)\n",
    "        with open(os.path.join(folder, \"scaler.pt\"), \"wb\") as f:\n",
    "            cloudpickle.dump(scaler,f)\n",
    "            \n",
    "    \n",
    "    eval = { \n",
    "        \"train_mae\" : [],\n",
    "        \"test_mae\" : [],\n",
    "        \"train_mse\" : [],\n",
    "        \"test_mse\" : [],\n",
    "        \"train_r2\" : [],\n",
    "        \"test_r2\" : []\n",
    "    }\n",
    "    \n",
    "    def format_eval():\n",
    "        return f\"MAE [{unit}]: {eval['test_mae'][-1]:.4f} ({eval['train_mae'][-1]:.4f}) - \"\\\n",
    "               f\"R²: {eval['test_r2'][-1]:.2f} ({eval['train_r2'][-1]:.2f})\"\n",
    "    \n",
    "    view_class = np.zeros_like(target) if view_class is None else view_class\n",
    "        \n",
    "    for i in range(5):\n",
    "        feats_cv, feats_test, target_cv, target_test, class_cv, class_test = \\\n",
    "            train_test_split(feats, target, view_class, random_state=1868+i, test_size=test_split, stratify=sample_class)\n",
    "        hypmodel = GridSearchCV(model(), params, \n",
    "                                cv=5, scoring=\"neg_mean_squared_error\", n_jobs=8)\n",
    "        hypmodel.fit(feats_cv, target_cv)\n",
    "        pred_cv = hypmodel.predict(feats_cv)\n",
    "        pred_test = hypmodel.predict(feats_test)\n",
    "        \n",
    "        for t, est, name in list(\n",
    "            [(target_cv, pred_cv, \"train\",), (target_test, pred_test, \"test\")]):\n",
    "            eval[f\"{name}_mae\"].append(mean_absolute_error(t, est))\n",
    "            eval[f\"{name}_mse\"].append(mean_squared_error(t, est))\n",
    "            eval[f\"{name}_r2\"].append(r2_score(t, est))\n",
    "            \n",
    "            \n",
    "        parityplot(os.path.join(folder, f\"pp{str(i).zfill(2)}.pdf\",),\n",
    "                  (target_cv, pred_cv, class_cv),\n",
    "                  (target_test, pred_test, class_test),\n",
    "                  title=format_eval(), xlabel=f\"{xlabel} [{unit}]\",\n",
    "                   ylabel=f\"prediction [{unit}]\",\n",
    "                  **pp_kws)\n",
    "        \n",
    "        with open(os.path.join(folder, f\"model{str(i).zfill(2)}.pt\"), \"wb\") as f:\n",
    "            cloudpickle.dump(hypmodel,f)\n",
    "            \n",
    "    with open(os.path.join(folder, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(eval, f)\n",
    "    \n",
    "    eval_agg = dict(\n",
    "        ((k, \n",
    "          (f\"{sum(v)/len(v):.4f}\", f\"{np.std(v):.4f}\")\n",
    "          ) for k, v in eval.items()))\n",
    "    return eval_agg\n",
    "\n",
    "\n",
    "def run_regressor_manual(feats_cv, target_cv,\n",
    "                         feats_test, target_test,\n",
    "                         model, params,\n",
    "                         name=\"test\", scaler=None, pp_kws=None):\n",
    "    pp_kws = {} if pp_kws is None else pp_kws\n",
    "    folder = f\"outputs_manual/{name}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    if scaler:\n",
    "        scaler = scaler()\n",
    "        scaler.fit(feats_cv)\n",
    "        feats_cv = scaler.transform(feats_cv)\n",
    "        feats_test = scaler.transform(feats_test)\n",
    "        with open(os.path.join(folder, \"scaler.pt\"), \"wb\") as f:\n",
    "            cloudpickle.dump(scaler,f)\n",
    "            \n",
    "    \n",
    "    eval = { \n",
    "        \"train_mae\" : [],\n",
    "        \"test_mae\" : [],\n",
    "        \"train_mse\" : [],\n",
    "        \"test_mse\" : [],\n",
    "        \"train_r2\" : [],\n",
    "        \"test_r2\" : []\n",
    "    }\n",
    "    \n",
    "    def format_eval():\n",
    "        return f\"mae: {eval['test_mae'][-1]:.4f} ({eval['train_mae'][-1]:.4f})\"\\\n",
    "               f\"R²: {eval['test_r2'][-1]:.2f} ({eval['train_r2'][-1]:.2f})\"\n",
    "        \n",
    "    hypmodel = GridSearchCV(model(), params, \n",
    "                     cv=5, scoring=\"neg_mean_squared_error\", n_jobs=8)\n",
    "    hypmodel.fit(feats_cv, target_cv)\n",
    "    pred_cv = hypmodel.predict(feats_cv)\n",
    "    pred_test = hypmodel.predict(feats_test)\n",
    "\n",
    "    for t, est, name in list(\n",
    "        [(target_cv, pred_cv, \"train\",), (target_test, pred_test, \"test\")]):\n",
    "        eval[f\"{name}_mae\"].append(mean_absolute_error(t, est))\n",
    "        eval[f\"{name}_mse\"].append(mean_squared_error(t, est))\n",
    "        eval[f\"{name}_r2\"].append(r2_score(t, est))\n",
    "\n",
    "\n",
    "    parityplot(os.path.join(folder, f\"pp.pdf\",),\n",
    "              (target_cv, pred_cv),\n",
    "              (target_test, pred_test),\n",
    "              title=format_eval(), **pp_kws)\n",
    "\n",
    "    with open(os.path.join(folder, f\"model.pt\"), \"wb\") as f:\n",
    "        cloudpickle.dump(hypmodel,f)\n",
    "\n",
    "    with open(os.path.join(folder, \"metrics.json\"), 'w') as f:\n",
    "        json.dump(eval, f)\n",
    "    \n",
    "    eval_agg = dict(\n",
    "        ((k, \n",
    "          (sum(v)/len(v), np.std(v))\n",
    "          ) for k, v in eval.items()))\n",
    "    return eval_agg\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, MultiOutputMixin\n",
    "\n",
    "class StratifiedMedianRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.lookup_table = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        x_flat = X.ravel()\n",
    "        for ux in np.unique(x_flat):\n",
    "            self.lookup_table[ux] = np.median(y[x_flat == ux])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        x_flat = X.ravel()\n",
    "        return np.array([self.lookup_table[ux] for ux in x_flat])\n",
    "    \n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        if X is None:\n",
    "            X = np.zeros(shape=(len(y), 1))\n",
    "        return super().score(X, y, sample_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
